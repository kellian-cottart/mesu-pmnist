# Bayesian continual learning and forgetting in neural networks - MNIST/PermutedMNIST Experiments

## Description

This repository holds the code for [Bayesian Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2504.13569), available on arXiv. It contains the experiments on the Permuted MNIST and MNIST OOD datasets, as well as the supplementary figures. The code is written in Python and uses PyTorch and Jax for the neural networks. 

The code for the CIFAR10-100, and Animals dataset is available [here](https://github.com/djo1996/Bayesian_Continual_Learning_And_Forgetting/tree/main) used to generate the figures in the paper.

## Table of contents

- [Bayesian continual learning and forgetting in neural networks - MNIST/PermutedMNIST Experiments](#bayesian-continual-learning-and-forgetting-in-neural-networks---mnistpermutedmnist-experiments)
  - [Description](#description)
  - [Table of contents](#table-of-contents)
  - [Environment installation](#environment-installation)
  - [Main figures](#main-figures)
    - [Figure 3](#figure-3)
    - [Figure 4](#figure-4)
  - [Supplementary figures](#supplementary-figures)
    - [Supplementary Note 6 - Impact of the memory window N on the Permuted MNIST dataset](#supplementary-note-6---impact-of-the-memory-window-n-on-the-permuted-mnist-dataset)
    - [Supplementary Note 9: Comparison with Presynaptic Consolidation](#supplementary-note-9-comparison-with-presynaptic-consolidation)
    - [Supplementary Note 10: Comparison with Uncertainty-guided Continual Bayesian Neural Networks (UCB)](#supplementary-note-10-comparison-with-uncertainty-guided-continual-bayesian-neural-networks-ucb)
    - [Supplementary Note 8: Inference time and accuracy, Supplementary Note 11: Detailed trade-off analysis of MESU and FOO-VB Diagonal](#supplementary-note-8-inference-time-and-accuracy-supplementary-note-11-detailed-trade-off-analysis-of-mesu-and-foo-vb-diagonal)
  - [Notebooks](#notebooks)
    - [Main figures](#main-figures-1)
    - [Supplementary figures](#supplementary-figures-1)
  - [Authors](#authors)
  - [Citation](#citation)
  - [License](#license)


## Environment installation

To install the environment needed to run the code, please use Conda/Mamba/Micromamba. Make sure you are running either of the commands in the main directory.

```bash
conda env create -f environment.yml
conda activate bayesian
```

The environment file includes all dependencies needed to run the code, including PyTorch, Jax, and their respective CUDA versions. The environment is set up for Python 3.12 and uses CUDA 12.1.

```yaml
name: bayesian
channels:
  - pytorch
  - nvidia
  - defaults
dependencies:
  - python=3.12
  - nvidia::cudatoolkit
  - pytorch-cuda=12.1
  - matplotlib
  - torchvision=0.18.0=py312_cu121
  - pip
  - pip:
      - idx2numpy
      - tqdm
      - jax[cuda12]==0.5.3
      - equinox
      - optax
      - seaborn
      - gdown
```

Datasets will be automatically downloaded when running the experiments in the `datasets` folder. The datasets are downloaded from the official sources, and the code will check if they are already present before downloading them again.

## Main figures

The main file is `main.py`. It is used to run the experiments according to the configuration file. The configuration file is a JSON file that contains the parameters for the experiment. Namely, the network architecture, the hyperparameters, the dataset, the optimizer. Here are the different arguments that can be passed to the main file:

* `-c` or `--config_file`: The path to the configuration file;
* `-it` or `--n_iterations`: The number of iterations to run the experiment for;
* `-v` or `--verbose`: Whether to display the progress bar or not;
* `-ood` or `--ood`: The dataset to compute the OOD on. Available datasets are `fashion`, `pmnist`, `None`;
* `-gpu` or `--gpu`: The GPU ID to use;
* `-wh` or `--weight_histogram`: Whether to save the weight histograms at each epochs;
* `-fits` or `--fits_in_memory`: Whether the dataset fits in GPU memory or not. Else, the dataset is loaded in RAM.

Configuration files are provided and located in the `configurations` folder. The configuration files are named according to the dataset and the network architecture. For example, `pmnist/mlp-sgd.json` is the configuration file for a multi-layer perceptron trained with stochastic gradient descent on the Permuted MNIST dataset. They hold the hyperparameters for the experiment, such as the learning rate, the batch size, the number of epochs, etc. Modifying these files allows you to change the hyperparameters of the experiment. The configuration files are used as input to the main file to run the experiments.

### Figure 3

The dataset used for OOD tests is Fashion-MNIST. The figure is generated by running the following commands. Each command runs for about 50 minutes for non-Bayesian neural networks and 2h30 for Bayesian networks on a RTX 3090.

```bash
python main.py -it 5 -ood fashion -c pmnist/mlp-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-online-ewc-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-stream-ewc-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-bayesian-mesu -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-bayesian-foovbdiagonal -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-si-sgd -fits
```

### Figure 4

The dataset used for the OOD tests is Permuted MNIST. These are the commands for Fig. 4. Each command runs for about 50 minutes for non-Bayesian neural networks and 2h30 for Bayesian networks on a RTX 3090.

```bash
python main.py -it 5 -ood pmnist -wh -c mnist-ood/mlp-sgd -fits
python main.py -it 5 -ood pmnist -wh -c mnist-ood/mlp-stream-ewc-sgd -fits
python main.py -it 5 -ood pmnist -wh -c mnist-ood/mlp-bayesian-foovbdiagonal -fits
python main.py -it 5 -ood pmnist -wh -c mnist-ood/mlp-bayesian-mesu -fits
python main.py -it 5 -ood pmnist -wh -c mnist-ood/mlp-bayesian-mesu-high-N -fits
```

## Supplementary figures

### Supplementary Note 6 - Impact of the memory window N on the Permuted MNIST dataset

The following commands generate the simulations for Supplementary Note 6. Once the simulations are done, and the results must be moved in the `RESULTS-N-STUDY` folder, the notebook `figure-pmnist-N-study.ipynb` can be used to generate the following figures:
- Supplementary Figure 2. Effect of the memory window N on accuracy; 
- Supplementary Figure 3. Joint influence of N and model width;
- Supplementary Figure 4. End-of-training variance distributions.
 
Depending on the size of the network, each command runs for about 30 minutes to 4 hours on a RTX 3090.

```bash
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=180000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=300000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=600000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=1200000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=2400000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=50-N=1e15 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=180000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=300000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=600000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=1200000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=2400000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=256-N=1e15 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=180000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=300000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=600000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=120000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=2400000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=512-N=1e15 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=180000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=300000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=600000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=1200000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=2400000 -fits
python main.py -it 5 -ood fashion -wh -c supp-size-study/mlp-bayesian-mesu-S=1024-N=1e15 -fits
```

### Supplementary Note 9: Comparison with Presynaptic Consolidation

The following command generate the simulations for the supplementary table on presynaptic consolidation. Once the simulations are done, and the results must be moved in the `RESULTS-PRESYNAPTIC` folder, the notebook `figure-pmnist-presynaptic.ipynb` can be used to generate Supplementary Table 4. Comparison of Metaplasticity from Synaptic Uncertainties (MESU) and Presynaptic Consolidation.

```bash
python main.py -it 5 -c supp-presynaptic/mlp-bayesian-mesu -fits
```

### Supplementary Note 10: Comparison with Uncertainty-guided Continual Bayesian Neural Networks (UCB)

The following command generate the simulations for the supplementary table on UCB. Once the simulations are done, and the results must be moved in the `RESULTS-UCB` folder, the notebook `figure-pmnist-UCB.ipynb` can be used to generate Supplementary Table 6. Comparison of Metaplasticity from Synaptic Uncertainties (MESU) and Uncertainty-guided Continual Bayesian Neural Networks (UCB).

```bash
python main.py -it 10 -c supp-ucb/mlp-bayesian-mesu-small -fits
```

### Supplementary Note 8: Inference time and accuracy, Supplementary Note 11: Detailed trade-off analysis of MESU and FOO-VB Diagonal

The simulations to be run are the same as in the main Permuted MNIST figure. The results must be saved in the `RESULTS-PMNIST` folder. The notebook `figure-pmnist-tradeoff.ipynb` can then be used to generate Supplementary Figure 5. Inference time and accuracy, and Supplementary Figure 6. Detailed trade-off analysis of MESU and FOO-VB Diagonal.

```bash
python main.py -it 5 -ood fashion -c pmnist/mlp-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-online-ewc-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-stream-ewc-sgd -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-bayesian-mesu -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-bayesian-foovbdiagonal -fits
python main.py -it 5 -ood fashion -c pmnist/mlp-si-sgd -fits
```

## Notebooks

### Main figures

Two files are available in the repository:

- [Figure 3](./figure-pmnist.ipynb)
- [Figure 4](./figure-mnist-ood.ipynb)

These two files are linked with two folders, respectively `RESULTS-PMNIST` and `RESULTS-MNIST-OOD`. To retrieve the figures, these folders must contain the results of the experiments previously generated with the figure commands.

The resulting figures will be saved in a folder named `output-figures` in the main directory.

### Supplementary figures

The supplementary figures are generated in the same way as the main figures. The notebooks are:
- [Supplementary Note 6 - Impact of the memory window N on the Permuted MNIST dataset](./figure-pmnist-N-study.ipynb)
- [Supplementary Note 9: Comparison with Presynaptic Consolidation](./figure-pmnist-presynaptic.ipynb)
- [Supplementary Note 10: Comparison with Uncertainty-guided Continual Bayesian Neural Networks (UCB)](./figure-pmnist-UCB.ipynb)
- [Supplementary Note 8: Inference time and accuracy, Supplementary Note 11: Detailed trade-off analysis of MESU and FOO-VB Diagonal](./figure-pmnist-tradeoff.ipynb)

The results of the experiments must be saved respectively under `RESULTS-N-STUDY` for Supp. Note 6, in `RESULTS-PRESYNAPTIC` for Supp. Note 9, `RESULTS-UCB` for Supp. Note 10. For Supp. Note 8 and 11, the results are the same as for the main figure, and must as such be saved in `RESULTS-PMNIST`.

The resulting figures will be saved in a folder named `output-figures` in the main directory.

## Authors

Main contributors:
- [Djohan BONNET](https://scholar.google.com/citations?user=1cSwOPIAAAAJ&hl=en)
- [Kellian COTTART](https://scholar.google.com/citations?hl=en&user=Akg-AH4AAAAJ)
  
Research director:
- [Damien QUERLIOZ](https://scholar.google.com/citations?user=2-EKdW4AAAAJ&hl=en)

## Citation

Please reference this work as

```bibtex
@misc{bonnet2025bayesiancontinuallearningforgetting,
      title={Bayesian continual learning and forgetting in neural networks}, 
      author={Djohan Bonnet and Kellian Cottart and Tifenn Hirtzlin and Tarcisius Januel and Thomas Dalgaty and Elisa Vianello and Damien Querlioz},
      year={2025},
      eprint={2504.13569},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2504.13569},
}
```

## License

This project is licensed under the CC-BY License - see the [LICENSE](LICENSE) file for details.
